{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "imperial-company",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "informative-scheduling",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from matenn.utils import *\n",
    "from matenn.data import BatchIterator\n",
    "from matenn.layers import Linear, Sigmoid, sigmoid\n",
    "from matenn.nn import NeuralNet\n",
    "from matenn.loss import MSE\n",
    "from matenn.optim import SGD\n",
    "from matenn.train import Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beginning-chamber",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "covered-plasma",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: (506, 13) \n",
      "Target: (506,) \n",
      "Features: (13,)\n"
     ]
    }
   ],
   "source": [
    "# Read\n",
    "boston = load_boston()\n",
    "data = boston.data\n",
    "target = boston.target\n",
    "features = boston.feature_names\n",
    "\n",
    "print(\"Data:\", data.shape, \"\\n\"\n",
    "      \"Target:\", target.shape, \"\\n\"\n",
    "      \"Features:\", features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "legendary-alberta",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "s = StandardScaler()\n",
    "data = s.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "meaningful-andrew",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (354, 13) \n",
      "y_train: (354, 1) \n",
      "X_test: (152, 13) \n",
      "y_test: (152, 1)\n"
     ]
    }
   ],
   "source": [
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, \n",
    "                                                    target, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=80718)\n",
    "\n",
    "y_train, y_test = to_2d_np(y_train), to_2d_np(y_test)\n",
    "\n",
    "print(\"X_train:\", X_train.shape, \"\\n\"\n",
    "      \"y_train:\", y_train.shape, \"\\n\"\n",
    "      \"X_test:\", X_test.shape, \"\\n\"\n",
    "      \"y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-monitoring",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "beautiful-writing",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = Trainer(iterator=BatchIterator(shuffle=False),\n",
    "             net=NeuralNet([\n",
    "                 Linear(input_size=13, output_size=13, seed=20190501),\n",
    "                 Sigmoid(),\n",
    "                 Linear(input_size=13, output_size=1, seed=20190501)\n",
    "             ]),\n",
    "             loss=MSE(),\n",
    "             optimizer=SGD())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heavy-minority",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "unavailable-darwin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 |  Train loss: 252.057 | Validation loss: 27.616\n",
      "Epoch: 20 |  Train loss: 187.327 | Validation loss: 21.258\n",
      "Epoch: 30 |  Train loss: 157.553 | Validation loss: 18.767\n",
      "Epoch: 40 |  Train loss: 141.563 | Validation loss: 17.399\n",
      "Epoch: 50 |  Train loss: 130.301 | Validation loss: 16.556\n"
     ]
    }
   ],
   "source": [
    "nn.fit(X_train=X_train, y_train=y_train,\n",
    "       X_test=X_test, y_test=y_test,\n",
    "       epochs=50,\n",
    "       eval_every=10,\n",
    "       seed=20190501)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-individual",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-mainland",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "grateful-parent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z1: (354, 13) \n",
      "a1: (354, 13) \n",
      "z2: (354, 1)\n"
     ]
    }
   ],
   "source": [
    "# Initalize a Layer\n",
    "l1 = Linear(input_size=13, output_size=13)\n",
    "z1 = l1.forward(inputs=X_train)\n",
    "a1 = sigmoid(z1)\n",
    "l2 = Linear(input_size=13, output_size=1)\n",
    "z2 = l2.forward(inputs=a1)\n",
    "print(\"z1:\", z1.shape, \"\\n\"\n",
    "      \"a1:\", a1.shape, \"\\n\"\n",
    "      \"z2:\", z2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "frozen-alexandria",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize: Iterator\n",
    "iterator = BatchIterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "vietnamese-carnival",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize: NN\n",
    "net = NeuralNet([\n",
    "    Linear(input_size=13, output_size=13),\n",
    "    Sigmoid(),\n",
    "    Linear(input_size=13, output_size=1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dominant-richards",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize: Loss\n",
    "loss = MSE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "statistical-kidney",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize: Optimizer\n",
    "optimizer = SGD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "sensitive-ratio",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_every = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "third-cambodia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 |  Train loss: 71.640 | Validation loss: 13.635\n",
      "Epoch: 20 |  Train loss: 69.644 | Validation loss: 13.707\n",
      "Epoch: 30 |  Train loss: 68.123 | Validation loss: 13.772\n",
      "Epoch: 40 |  Train loss: 66.387 | Validation loss: 13.784\n",
      "Epoch: 50 |  Train loss: 64.879 | Validation loss: 13.812\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "for epoch in range(50):\n",
    "        loss_train = 0.0\n",
    "        for batch in iterator(X_train, y_train):\n",
    "            # 1. forward pass from layer to layer to get the predictions\n",
    "            pred_train = net.forward(batch.inputs)\n",
    "            # 2. training loss\n",
    "            loss_train += loss.loss(pred_train, batch.targets)\n",
    "            # 3. gradient of the loss function\n",
    "            grad = loss.grad(pred_train, batch.targets)\n",
    "            # 4. backward pass from layer to layer\n",
    "            net.backward(grad)\n",
    "            # 5. update parameters\n",
    "            optimizer.step(net)\n",
    "        # 6. validation\n",
    "        if (epoch + 1) % eval_every == 0:\n",
    "            pred_test = net.forward(X_test)\n",
    "            loss_test = loss.loss(pred_test, y_test)\n",
    "            print(f\"Epoch: {epoch + 1} |  Train loss: {loss_train:.3f} | Validation loss: {loss_test:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selected-glasgow",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "numeric-chosen",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"aaaabbbcca\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "parliamentary-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "character = string[0]\n",
    "counter = 0\n",
    "for i, v in enumerate(string):\n",
    "    if v == character:\n",
    "        counter += 1\n",
    "    else:\n",
    "        output.append((character, counter))\n",
    "        counter = 1\n",
    "        character = v\n",
    "    if i + 1 == len(string):\n",
    "        output.append((character, counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "motivated-count",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 4), ('b', 3), ('c', 2), ('a', 1)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "headed-formula",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
